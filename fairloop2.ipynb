{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9f084c93b1bc1abc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T04:54:53.048294Z",
     "start_time": "2024-07-18T04:52:33.541345Z"
    }
   },
   "cell_type": "code",
   "source": "pip install transformers[torch]",
   "id": "839fbb85069384f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pip install vaderSentiment\n",
   "id": "764516448da04626"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set up the model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device='cpu')\n",
    "\n",
    "# Load and prepare the dataset\n",
    "file_path = '/content/drive/MyDrive/bold.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df = df[df.domain == \"gender\"]\n",
    "\n",
    "# Fixed 2000 samples at the start of the experiment\n",
    "sample_size_per_category = 1000\n",
    "seed = 42\n",
    "\n",
    "# Randomly choose samples for male and female categories\n",
    "df_actors = df[df.category == \"American_actors\"].sample(n=sample_size_per_category, random_state=seed)\n",
    "df_actresses = df[df.category == \"American_actresses\"].sample(n=sample_size_per_category, random_state=seed)\n",
    "fixed_samples = pd.concat([df_actors, df_actresses]).reset_index(drop=True)\n",
    "\n",
    "# Function to generate synthetic data using pipeline\n",
    "def generate_synthetic_data(pipeline, prompts, max_length=25, min_words=15):\n",
    "    synthetic_data = []\n",
    "    for item in prompts:\n",
    "        generated_text = pipeline(item[\"text\"], min_length=min_words, max_new_tokens=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)[0]['generated_text']\n",
    "        if len(generated_text) > len(item[\"text\"]):\n",
    "            synthetic_data.append({\n",
    "                \"generation\": item[\"generation\"],\n",
    "                \"sample_id\": item[\"sample_id\"],\n",
    "                \"prompt\": item[\"text\"],\n",
    "                \"text\": generated_text[len(item[\"text\"]):].strip(),\n",
    "                \"demographic\": item[\"demographic\"],\n",
    "                \"category\": item[\"category\"]\n",
    "            })\n",
    "        else:\n",
    "            synthetic_data.append({\n",
    "                \"generation\": item[\"generation\"],\n",
    "                \"sample_id\": item[\"sample_id\"],\n",
    "                \"prompt\": item[\"text\"],\n",
    "                \"text\": \" \",\n",
    "                \"demographic\": item[\"demographic\"],\n",
    "                \"category\": item[\"category\"]\n",
    "            })\n",
    "    return synthetic_data\n",
    "\n",
    "# Prepare synthetic data for training\n",
    "def prepare_synthetic_data_for_training(synthetic_data):\n",
    "    train_texts = [item[\"text\"] for item in synthetic_data]\n",
    "    train_inputs = tokenizer(train_texts, truncation=True, padding=True, max_length=25, return_tensors=\"pt\")\n",
    "    train_labels = tokenizer(train_texts, truncation=True, padding=True, max_length=25, return_tensors=\"pt\").input_ids\n",
    "    return train_inputs, train_labels\n",
    "\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].to(device) for key, val in self.inputs.items()}\n",
    "        item['labels'] = self.labels[idx].to(device)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs[\"input_ids\"])\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # Custom loss computation using Cross-Entropy Loss\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Function to perform sentiment analysis\n",
    "def analyze_sentiments(data, sentiment_analyzer):\n",
    "    sentiment_results = []\n",
    "    for item in data:\n",
    "        if item[\"text\"].strip() == \"\":\n",
    "            continue\n",
    "        sentiment = sentiment_analyzer.polarity_scores(item[\"text\"])[\"compound\"]\n",
    "        sentiment_results.append({\n",
    "            'demographic': item['demographic'],\n",
    "            'category': item['category'],\n",
    "            'score': sentiment,\n",
    "            'sample_id': item['sample_id'],\n",
    "            'generation': item['generation']\n",
    "        })\n",
    "    return sentiment_results\n",
    "\n",
    "def analyze_regard(data, regard_model):\n",
    "    sentiment_results = []\n",
    "    for item in data:\n",
    "        if item[\"text\"].strip() == \"\":\n",
    "            continue\n",
    "        regard_score = regard_model(item[\"text\"])\n",
    "        print(f\"Regard score for text '{item['text']}': {regard_score}\")  # Debugging line\n",
    "\n",
    "        # Assuming the format of regard_score is a list of dictionaries, based on common output formats for such models.\n",
    "        if isinstance(regard_score, list) and len(regard_score) > 0 and isinstance(regard_score[0], dict):\n",
    "            if any(entry.get('label') == 'positive' for entry in regard_score):\n",
    "                positive_score = next((entry['score'] for entry in regard_score if entry['label'] == 'positive'), 0.0)\n",
    "            else:\n",
    "                positive_score = 0.0\n",
    "        else:\n",
    "            positive_score = 0.0  # Default value if the format is not as expected\n",
    "\n",
    "        sentiment_results.append({\n",
    "            'demographic': item['demographic'],\n",
    "            'category': item['category'],\n",
    "            'score': positive_score,\n",
    "            'sample_id': item['sample_id'],\n",
    "            'generation': item['generation']\n",
    "        })\n",
    "    return sentiment_results\n",
    "\n",
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return perplexity\n",
    "\n",
    "# Function to plot sentiment score differences over generations\n",
    "def plot_score_differences(sentiment_history, regard_history, perplexity_history):\n",
    "    sentiment_df = pd.DataFrame(sentiment_history)\n",
    "    regard_df = pd.DataFrame(regard_history)\n",
    "    perplexity_df = pd.DataFrame(perplexity_history)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(sentiment_df['generation'], sentiment_df['difference'], label='Difference in VADER Sentiment Score')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Difference in VADER Sentiment Score')\n",
    "    plt.title('Difference in VADER Sentiment Score between American_actors and American_actresses')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(regard_df['generation'], regard_df['difference'], label='Difference in Regard Score')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Difference in Regard Score')\n",
    "    plt.title('Difference in Regard Score between American_actors and American_actresses')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(perplexity_df['generation'], perplexity_df['average_perplexity'], label='Average Perplexity')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Average Perplexity')\n",
    "    plt.title('Average Perplexity over Generations')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Main loop to run multiple generations\n",
    "num_generations = 10\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "regard_analyzer = pipeline(\"text-classification\", model=\"sasha/regardv3\", device='cpu')\n",
    "sentiment_history = []\n",
    "regard_history = []\n",
    "perplexity_history = []\n",
    "generated_texts = []\n",
    "previous_synthetic_data = None\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    print(f\"Generation {generation + 1}\")\n",
    "\n",
    "    current_data = fixed_samples.copy()\n",
    "    current_data = current_data.assign(generation=generation, sample_id=current_data.index)\n",
    "    current_data = current_data.to_dict(orient='records')\n",
    "    current_data = [{\"generation\": item[\"generation\"], \"sample_id\": item[\"sample_id\"], \"text\": item[\"prompts\"], \"demographic\": item[\"domain\"], \"category\": item[\"category\"]} for item in current_data]\n",
    "\n",
    "    # Generate synthetic data\n",
    "    synthetic_data = generate_synthetic_data(text_generator, current_data)\n",
    "    generated_texts.append(pd.DataFrame(synthetic_data))\n",
    "\n",
    "    # Fine-tune the model with synthetic data from the previous generation\n",
    "    if previous_synthetic_data is not None:\n",
    "        train_inputs, train_labels = prepare_synthetic_data_for_training(previous_synthetic_data)\n",
    "        train_dataset = SimpleDataset(train_inputs, train_labels)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            num_train_epochs=10,\n",
    "            per_device_train_batch_size=64,\n",
    "            save_steps=10_000,\n",
    "            save_total_limit=2,\n",
    "            logging_steps=100\n",
    "        )\n",
    "\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device='cpu')\n",
    "\n",
    "    previous_synthetic_data = synthetic_data\n",
    "\n",
    "    # Perform sentiment analysis on the current generation\n",
    "    sentiment_results = analyze_sentiments(synthetic_data, sentiment_analyzer)\n",
    "    regard_results = analyze_regard(synthetic_data, regard_analyzer)\n",
    "\n",
    "    # Calculate perplexity for each sample\n",
    "    perplexities = [calculate_perplexity(model, tokenizer, item['text']) for item in synthetic_data if item['text'].strip() != \"\"]\n",
    "    average_perplexity = sum(perplexities) / len(perplexities) if perplexities else float('inf')\n",
    "\n",
    "    # Store sentiment, regard, and perplexity results\n",
    "    for item in synthetic_data:\n",
    "        item['vader_score'] = next((res['score'] for res in sentiment_results if res['sample_id'] == item['sample_id']), None)\n",
    "        item['regard_score'] = next((res['score'] for res in regard_results if res['sample_id'] == item['sample_id']), None)\n",
    "        item['perplexity'] = calculate_perplexity(model, tokenizer, item['text']) if item['text'].strip() != \"\" else None\n",
    "\n",
    "    generated_texts[-1] = pd.DataFrame(synthetic_data)\n",
    "\n",
    "    # Calculate and store sentiment score differences for each generation\n",
    "    actors_scores = [res['score'] for res in sentiment_results if res['category'] == \"American_actors\"]\n",
    "    actresses_scores = [res['score'] for res in sentiment_results if res['category'] == \"American_actresses\"]\n",
    "    avg_actors_score = sum(actors_scores) / len(actors_scores) if actors_scores else 0\n",
    "    avg_actresses_score = sum(actresses_scores) / len(actresses_scores) if actresses_scores else 0\n",
    "    score_difference = avg_actors_score - avg_actresses_score\n",
    "\n",
    "    sentiment_history.append({\n",
    "        'generation': generation,\n",
    "        'actors': avg_actors_score,\n",
    "        'actresses': avg_actresses_score,\n",
    "        'difference': score_difference\n",
    "    })\n",
    "\n",
    "    # Calculate and store regard score differences for each generation\n",
    "    actors_regard_scores = [res['score'] for res in regard_results if res['category'] == \"American_actors\" and res['score'] is not None]\n",
    "    actresses_regard_scores = [res['score'] for res in regard_results if res['category'] == \"American_actresses\" and res['score'] is not None]\n",
    "    avg_actors_regard_score = sum(actors_regard_scores) / len(actors_regard_scores) if actors_regard_scores else 0\n",
    "    avg_actresses_regard_score = sum(actresses_regard_scores) / len(actresses_regard_scores) if actresses_regard_scores else 0\n",
    "    regard_score_difference = avg_actors_regard_score - avg_actresses_regard_score\n",
    "\n",
    "    regard_history.append({\n",
    "        'generation': generation,\n",
    "        'actors': avg_actors_regard_score,\n",
    "        'actresses': avg_actresses_regard_score,\n",
    "        'difference': regard_score_difference\n",
    "    })\n",
    "\n",
    "    perplexity_history.append({\n",
    "        'generation': generation,\n",
    "        'average_perplexity': average_perplexity\n",
    "    })\n",
    "\n",
    "# Save the generated texts to CSV\n",
    "all_generated_texts = pd.concat(generated_texts)\n",
    "all_generated_texts.to_csv(\"generated_texts_across_generations2.csv\", index=False)\n",
    "\n",
    "# Plot score differences over generations\n",
    "plot_score_differences(sentiment_history, regard_history, perplexity_history)\n"
   ],
   "id": "3a4e2b2b80231544"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
